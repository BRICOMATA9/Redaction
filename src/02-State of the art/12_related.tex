\subsection{MADM}




\subsection{Learning}

\subsubsection{Bandit Algorithm}

\begin{frame}{Multi-Armed-Bandit Algorithm}{Related work}
\begin{itemize}
	\item \bf{Arms:} K = {1, ... , K}
	\item \bf{Decision:} T = {1, ... , T}
	\item \bf{Reward:} $X^{k}_{t}$ with $\mu^{k}_{t}$ = E $[X^{k}_{t}]$
	\begin{itemize}
		\item \bf{Best reward:} $X^{*}_{t}$ with $\mu^{*}_{t}$ = max $\mu^{k}_{t}$,  k\in  K
	\end{itemize}
\end{itemize}
\end{frame}

\subsubsection{Genetic Algorithm}

\begin{frame}{Genetic Algorithm}{Related work \cite{alkhawlani_access_2008a}}
\begin{itemize}
	\item \textbf{N} transceiver configurations: ($x_{1}, ..., x_{n}$)
	\item \textbf{I} QoS metrics ($m_{1}, ..., m_{i}$). ex: the operators, the applications, and the network conditions.
	\item \textbf{I} weights ($w_{1}, ..., w_{i}$) are sent to the MCDM in the second component.
	\item GA component assigns a suitable weight (w 1 ,w 2 ,...,w i )
\end{itemize}
\end{frame}


\begin{frame}{Genetic Algorithm}{Related work}
% \begin{itemize}
	% \item Evaluation function
		\begin{tabular}{ll}
			\multicolumn{2}{l}{\textbf{Evaluation function}} \\\\
			Define the number of parameters                       	& \{SF, Tx, CR, BW\} \\
			Define the target QoS			                      	& \{RSSI, SNR, delay, PDR, RTD\} \\
			Define evaluation function                            	& \red{Score}(SF, Tx, CR, BW) -> \{RSSI, SNR, delay, PDR, RTD\} \\\\
			\multicolumn{2}{l}{\textbf{Parameters}} \\\\
			Define a population of individuals (solutions)      	& 6720  \\
			Define probabilities of crossing and mutating       	& 0.5, 0.2 \\ 
			Define the number of generations                   	 	& 60 \\\\
			\multicolumn{2}{l}{\textbf{Generations}} \\\\
			Select individuals randomly    							& \{$SF_{i}, Tx_{i}, CR_{i}, BW_{i}\}^{random}$ 	\\
			Clone, crossover and mutate this individuals 	        & \{$SF_{i+1}, Tx_{i+1}, CR_{i+1}, BW_{i+1}\}^{random}$ \\
			Evaluate the offspring with an invalid Fitness 			& \red{Score}($SF_{i+1}, Tx_{i+1}, CR_{i+1}, BW_{i+1}$) \\\\
			\multicolumn{2}{l}{\textbf{(Crossover, Mutation)}} \\\\
			Remove some bad solutions                                & \\
			Duplicate some good solutions                            & \\
			Make small changes to some of them                       & \\
		\end{tabular}


% 	\item Genetic algorithm parameters:
% 	\begin{itemize}

% 	\end{itemize}
% 	\item : 
% 	\begin{itemize}
% 		\item Evaluate the entire population 						-> fitness
% 	\end{itemize}
% 	\item  : (60)
% 	\begin{itemize}

% 	\end{itemize}
% \end{itemize}
\end{frame}

\subsubsection{Q-Learning}


\begin{frame}{Marcov chain}{Related work}
\Figure{h}{.5}{qlearning}{qlearning}
\end{frame}


\subsubsection{Marcov Chain}

\begin{frame}{Marcov chain}{Related work}

\begin{flushleft}
\begin{equation}
V(s, \pi)=\mathbb{E}_{s}^{\pi}\left(\sum_{k=0}^{\mathrm{inf}} \gamma^{k} \cdot r\left(s_{k}, a_{k}\right)\right), s \in \mathbb{S}
\end{equation}

\begin{equation}
r\left(s_{k}, a_{k}\right)=G_{k} \cdot P R R\left(a_{k}\right)
\end{equation}

\begin{equation}
\pi^{*}=\arg \max _{\pi} V(s, \pi)
\end{equation}

% \stamp{HGHGJ}
% \begin{tikzpicture}[remember picture, overlay]
% 	\node[draw, rotate=30] at (25em, 7ex) {\color{red!90}\huge\bfseries APPROVED};
% \end{tikzpicture}

\begin{equation}
PRR=(1-BER)^{L}
\end{equation}

\begin{equation}
BER=10^{\alpha e^{\beta SNR}}
\end{equation}

\end{flushleft}

\end{frame}

\begin{frame}{Marcov chain}{Related work}

\Columns{0.6}{.4}{

Learning iterative steps:

\Itemize{
	\item \textbf{Choose} action $a_{k}(t) \sim \pi_{k}(t)$
	\item \textbf{Observe} game outcome 
	\Itemize{
		\item $a_{\_k}(t)$
		\item $u_{k}(a_{k}(t), a_{\_k}(t))$
	}
	\item \textbf{Improve} $\pi_{k}(t+1)$
}

Thus, we can expect that \forall k \in K

\Equation{e1}{\pi_{k(t)} \xrightarrow{t\longrightarrow\infty} \pi^{*}_{k}}
\Equation{e2}{u_{k}(\pi_{k}(t), \pi_{\_k}(t)) \xrightarrow{t\longrightarrow\infty} u_{k}(\pi^{*}_{k}, \pi^{*}_{\_k})}

Where:
\Itemize{
	\item $ \pi^{*} = (\pi^{*}_{1}, ..., \pi^{*}_{k})$ is the NE strategy profile
}

}{
	\Figure{h}{1}{marcov}{}
}

\end{frame}














\subsection{Fuzzy Logic}

\subsection{Utility Function}



\subsection{Game Theory}
\begin{frame}{Game theory}{Related work}
\Itemize{
	\item \bf{Players:} $K = \{1, ... , K\}$
	\item \bf{Strategies:} $S =S_{1} \times \ldots \times S_{K}$
	\Itemize{
		\item $S_{k}$ is the strategy set of the $k^{th}$ player.
	}
	\item \bf{Rewards:} $u_{k} : S \longrightarrow R_{+}$ and is denoted by $r_{k} (s_{k} , s_{-k})$
	\Itemize{
		\item $s_{-k}=\left(s_{1}, \dots, s_{k-1}, s_{k+1}, \ldots, s_{K}\right) \in S_{1} \times \ldots \times S_{k-1} \times S_{k+1} \times \ldots \times S_{K}$
	}
}
 \end{frame}


